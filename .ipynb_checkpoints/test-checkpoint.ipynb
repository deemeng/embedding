{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "88097f44-cf30-4994-96c0-9b66bbd31016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x714c417a1490>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import esm\n",
    "\n",
    "import re\n",
    "import gc\n",
    "\n",
    "from utils.common import load_tab, save_np, load_np, read_json2list\n",
    "from utils.alignmentParser import read_msa, greedy_select\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d13110b3-71b8-423b-81e1-08998c4251a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import params as p\n",
    "\n",
    "from Bio import SeqIO\n",
    "from utils.common import dump_list2json\n",
    "path_input_dataset_json = p.path_input_dataset_json\n",
    "path_output_features_msaTrans = p.path_output_features_msaTrans\n",
    "path_hmm = p.path_hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90918c04-a8d4-41de-b169-51abb1884886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3669c340-4230-4b1a-a6e6-f3eb70006fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input JSON file is created under: /home/dimeng/caid3/embedding/data/dataset.json\n"
     ]
    }
   ],
   "source": [
    "# get FASTA file\n",
    "fasta_sequences = SeqIO.parse(open(p.path_input),'fasta')\n",
    "list_entity = []\n",
    "for entity in fasta_sequences:\n",
    "    dict_e = {}\n",
    "    dict_e['id'], dict_e['sequence'] = entity.id, str(entity.seq)\n",
    "    list_entity.append(dict_e)\n",
    "\n",
    "# save JSON file\n",
    "dump_list2json(list_entity, path_input_dataset_json)\n",
    "print(f'input JSON file is created under: {path_input_dataset_json}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f81f5-5179-471c-86ea-4c14b01dcbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf99956-8268-4e15-8794-9f29666111ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ba773b-97a3-416d-a738-3bbce9cbfedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d613ffc-0b36-47a1-8296-b863743023e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP02585\n"
     ]
    }
   ],
   "source": [
    "name = 'DP02585'\n",
    "print(name)\n",
    "# This is where the data is actually read in\n",
    "inputs = read_msa(os.path.join(path_hmm, f'{name}.a3m'))\n",
    "\n",
    "inputs = greedy_select(inputs, num_seqs=128) # can change this to pass more/fewer sequences\n",
    "msa_transformer_batch_labels, msa_transformer_batch_strs, msa_transformer_batch_tokens = msa_transformer_batch_converter([inputs])\n",
    "msa_transformer_batch_tokens = msa_transformer_batch_tokens.to(next(msa_transformer.parameters()).device)\n",
    "# Extract per-residue representations (on CPU)\n",
    "with torch.no_grad():\n",
    "    results = msa_transformer(msa_transformer_batch_tokens, repr_layers=[12], return_contacts=False)\n",
    "token_representations = results[\"representations\"][12]\n",
    "seq_representation = token_representations[:, :, 1: ].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1289434a-e137-4b46-8cce-571fcf135418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 591, 768])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_representation.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4269e845-fbc5-47aa-bca1-b5df8bce1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_rep = seq_representation.detach().cpu().numpy()\n",
    "seq_rep = np.concatenate([seq_rep, seq_rep], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "56af80bf-00c9-4c1a-8609-a1ed68219ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_empty = np.empty([1, length, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f2919b78-a847-4049-892c-294835a5d97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.20227243, -0.11333056,  0.2251099 ,  0.24856019,\n",
       "         -1.0699196 ],\n",
       "        [-0.5465641 , -0.15934734,  0.3027207 ,  0.21070966,\n",
       "         -1.1074893 ]]], dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(seq_rep[:, -2:, :5] + seq_rep[:, -2:, :5])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "be61c4b7-290d-4545-94df-bae9deeec893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18389681,  0.13333188, -0.43770298, -0.78563946,  0.17937823,\n",
       "        -0.6691764 ,  0.24237913,  0.26934707, -0.84384835]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_rep[:, 3*2-2:3*5-2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "28c2ff98-4b4c-4b20-a1e5-13a9f48b096d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18389681,  0.13333188, -0.43770298, -0.78563946,  0.17937823,\n",
       "        -0.6691764 ,  0.24237913,  0.26934707, -0.84384835]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_rep[:, (3*2-2):(3*5-2), 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8611f7ae-2f0d-4634-a82a-8b69c6807b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _msaTrans_long(length: int, name: str, path_hmm: str):\n",
    "    '''\n",
    "    params:\n",
    "        length - int, sequence length\n",
    "        name - protein ID, for reading NAME.a3m file\n",
    "        path_hmm - folder dir to .a3m files\n",
    "\n",
    "    return:\n",
    "        embed_seq - numpy array, shape: (1,length ,768), where 768 is number of features\n",
    "    '''\n",
    "    embed_seq = np.empty([1, length, 768])\n",
    "    \n",
    "    # load the long sequence\n",
    "    inputs = read_msa(os.path.join(path_hmm, f'{name}.a3m'))\n",
    "    inputs = greedy_select(inputs, num_seqs=128)\n",
    "    _, _, msa_transformer_batch_tokens = msa_transformer_batch_converter([inputs])\n",
    "    msa_transformer_batch_tokens = msa_transformer_batch_tokens.to(next(msa_transformer.parameters()).device)\n",
    "    # separate the long sequence\n",
    "    for i in range(length//500):\n",
    "        # tail\n",
    "        if (i+2)*500>=length:\n",
    "            batch_token = msa_transformer_batch_tokens[:, :, [0]+list(range(i*500+1, length+1))]\n",
    "        else:\n",
    "            batch_token = msa_transformer_batch_tokens[:, :, [0]+list(range(i*500+1, i*500+1000+1))]\n",
    "        with torch.no_grad():\n",
    "            results = msa_transformer(batch_token, repr_layers=[12], return_contacts=False)\n",
    "        token_representations = results[\"representations\"][12]\n",
    "        seq_representation = token_representations[:, :, 1: ].mean(1)\n",
    "        seq_rep = seq_representation.detach().cpu().numpy()\n",
    "        # head\n",
    "        if i==0:\n",
    "            embed_seq = seq_rep\n",
    "        else:\n",
    "            seq_rep[:, 50:450, :] = (embed_seq[:, -450:-50, :] + seq_rep[:, 50:450, :])/2\n",
    "            # Here!!!!!!\n",
    "            embed_seq = np.concatenate((embed_seq[:, :-450, :], seq_rep[:, 50:, :]), axis=1) \n",
    "    return embed_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4270048e-bebb-442e-87ef-7588b45009a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "56d32dd2-3744-44b3-bd22-40efed6a9f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.DataFrame(read_json2list(path_input_dataset_json))\n",
    "df_dataset['length'] = [len(seq) for seq in df_dataset['sequence']]\n",
    "df_dataset = df_dataset[df_dataset['length']>20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b060341e-aacb-4357-9feb-1eb3d3535f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sequence length greater than 1022(1022 + start/end tokens)\n",
    "for idx, row in df_dataset.iterrows():\n",
    "    length = row['length']\n",
    "    name = row['id']\n",
    "    embed_seq = _msaTrans_long(length, name, path_hmm)\n",
    "    # save embedd sequences\n",
    "    save_np(embed_seq, os.path.join(path_output_features_msaTrans, f'{name}.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "465b3de3-30b8-4fa9-8630-618443cb4e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a181a-c090-4a7f-abd1-9e67feebd990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def msaTrans(path_input_dataset_json, path_output_features_msaTrans, path_hmm):\n",
    "    # 1. mdoel & tokenizer\n",
    "    msa_transformer, msa_transformer_alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "    msa_transformer = msa_transformer.eval()\n",
    "    msa_transformer_batch_converter = msa_transformer_alphabet.get_batch_converter()\n",
    "    \n",
    "    # 2. predict & save embedded sequences\n",
    "    df_dataset = pd.DataFrame(read_json2list(path_input_dataset_json))\n",
    "    df_dataset['length'] = [len(seq) for seq in df_dataset['sequence']]\n",
    "    df_dataset_long = df_dataset[df_dataset['length']>1022]\n",
    "    df_dataset = df_dataset[df_dataset['length']<=1022]\n",
    "\n",
    "    # 2.1. short sequences\n",
    "    # entyID_entityID\n",
    "    seq_IDS = list(set(df_dataset['id'].tolist()))\n",
    "    for name in seq_IDS:\n",
    "        print(name)\n",
    "        # This is where the data is actually read in\n",
    "        inputs = read_msa(os.path.join(path_hmm, f'{name}.a3m'))\n",
    "        \n",
    "        inputs = greedy_select(inputs, num_seqs=128) # can change this to pass more/fewer sequences\n",
    "        msa_transformer_batch_labels, msa_transformer_batch_strs, msa_transformer_batch_tokens = msa_transformer_batch_converter([inputs])\n",
    "        msa_transformer_batch_tokens = msa_transformer_batch_tokens.to(next(msa_transformer.parameters()).device)\n",
    "        # Extract per-residue representations (on CPU)\n",
    "        with torch.no_grad():\n",
    "            results = msa_transformer(msa_transformer_batch_tokens, repr_layers=[12], return_contacts=False)\n",
    "        token_representations = results[\"representations\"][12]\n",
    "        seq_representation = token_representations[:, :, 1: ].mean(1)\n",
    "        # save embedd sequences\n",
    "        save_np(seq_representation, os.path.join(path_output_features_msaTrans, f'{name}.npy'))\n",
    "\n",
    "    # 2.2. long sequences\n",
    "    # for sequence length greater than 1022(1022 + start/end tokens)\n",
    "    for idx, row in df_dataset_long.iterrows():\n",
    "        length = row['length']\n",
    "        name = row['id']\n",
    "        embed_seq = _msaTrans_long(length, name, path_hmm)\n",
    "        # save embedd sequences\n",
    "        save_np(embed_seq, os.path.join(path_output_features_msaTrans, f'{name}.npy'))\n",
    "    print('Done!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e94c7b-899b-4ceb-8068-b2afe8f906f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
